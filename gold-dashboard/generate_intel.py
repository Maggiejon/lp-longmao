#!/usr/bin/env python3
"""
è€é“ºé»„é‡‘ Â· å…¨çƒä¹°é‡‘ä¸“å‘˜æ‰‹å†Œ  Â·  è‡ªåŠ¨æ•°æ®æ›´æ–°è„šæœ¬

æ•°æ®æ¥æºï¼š
  å¸‚åœºæ•°æ®  â†’ yfinanceï¼ˆé‡‘ä»· / æ±‡ç‡ / 6181.HKï¼‰
  è´¢ç»æ–°é—»  â†’ yfinance 6181.HK + ä¸œæ–¹è´¢å¯Œ API + æ–°æµªè´¢ç»æœç´¢
  ç¤¾åª’åŠ¨æ€  â†’ å¾®åšæœç´¢ + æœç‹—å¾®ä¿¡ï¼ˆå…¬ä¼—å·ç´¢å¼•ï¼‰+ å°çº¢ä¹¦ï¼ˆPlaywright æ— å¤´æµè§ˆå™¨ï¼‰

å®šæ—¶è¿è¡Œï¼š
  .github/workflows/update-intel.yml  æ¯æ—¥ 18:00 CSTï¼ˆ10:00 UTCï¼‰è‡ªåŠ¨è§¦å‘
  æ‰‹åŠ¨è¿è¡Œï¼špython generate_intel.py
"""

import os
import re
import html as html_lib
from datetime import datetime, timezone, timedelta

# â”€â”€ å…¨å±€å¸¸é‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

CST = timezone(timedelta(hours=8))

KEYWORDS_ADJUST  = ["è°ƒä»·", "æ¶¨ä»·", "æä»·", "é™ä»·", "ä»·æ ¼è°ƒæ•´", "ä¸Šè°ƒ", "ä¸‹è°ƒ"]
KEYWORDS_PROMO   = ["ä¿ƒé”€", "å¤§ä¿ƒ", "ä¼˜æƒ ", "æŠ˜æ‰£", "æ»¡å‡", "æ´»åŠ¨", "ä¸“åœº", "é™æ—¶", "ç§’æ€"]
KEYWORDS_FINANCE = ["è´¢æŠ¥", "ä¸šç»©", "è¥æ”¶", "åˆ©æ¶¦", "IPO", "è‚¡ä¸œ", "è‚¡æƒ", "åˆ†çº¢", "å›è´­", "è¯„çº§"]

HEADERS_BROWSER = {
    "User-Agent": (
        "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) "
        "AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1"
    ),
    "Accept-Language": "zh-CN,zh;q=0.9",
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ä¸€ã€å¸‚åœºæ•°æ®
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def fetch_market_data() -> dict:
    """é€šè¿‡ yfinance è·å–é‡‘ä»·ã€æ±‡ç‡ã€6181.HK è‚¡ä»·"""
    try:
        import yfinance as yf

        def _close(ticker_str, period="5d"):
            h = yf.Ticker(ticker_str).history(period=period)
            if h.empty:
                raise ValueError(f"{ticker_str} æ— æ•°æ®")
            return h["Close"].dropna()

        gold_s   = _close("GC=F")
        cny_s    = _close("USDCNY=X")
        hk_s     = _close("6181.HK")

        gold_price  = gold_s.iloc[-1]
        gold_prev   = gold_s.iloc[-2] if len(gold_s) >= 2 else gold_price
        gold_change = (gold_price - gold_prev) / gold_prev * 100

        usdcny = cny_s.iloc[-1]

        hk_price  = hk_s.iloc[-1]
        hk_prev   = hk_s.iloc[-2] if len(hk_s) >= 2 else hk_price
        hk_change = (hk_price - hk_prev) / hk_prev * 100

        return {
            "gold_spot":   f"{gold_price:,.0f}",
            "gold_change": gold_change,
            "gold_note":   ("æœ¬å‘¨æŒç»­ä¸Šæ¶¨" if gold_change >= 1 else
                            "å°å¹…ä¸Šæ¶¨"     if gold_change >= 0 else
                            "å°å¹…å›è°ƒ"     if gold_change >= -1 else "æ˜æ˜¾å›è°ƒ"),
            "gold_cny":    f"{gold_price * usdcny / 31.1035:,.0f}",
            "usd_cny":     f"{usdcny:.4f}",
            "hk_price":    f"{hk_price:,.1f}",
            "hk_change":   f"{hk_change:+.2f}",
            "hk_color":    "green" if hk_change >= 0 else "red",
            "error":       None,
        }

    except Exception as e:
        print(f"  [WARN] å¸‚åœºæ•°æ®è·å–å¤±è´¥ï¼š{e}")
        return {
            "gold_spot": "è·å–ä¸­", "gold_change": 0, "gold_note": "è¯·ç¨ååˆ·æ–°",
            "gold_cny": "â€”", "usd_cny": "â€”", "hk_price": "â€”",
            "hk_change": "â€”", "hk_color": "neutral", "error": str(e),
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  äºŒã€è´¢ç»æ–°é—»ï¼ˆ3 ä¸ªæ¥æº â†’ æ³¨å…¥ã€Œä»·æ ¼æƒ…æŠ¥ã€å’Œã€Œé—¨åº—ä¿ƒé”€ã€Tabï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _make_news_item(title, source, link, pub_dt, channel) -> dict:
    return {"title": title, "source": source, "link": link,
            "pub_dt": pub_dt, "channel": channel, "category": classify_text(title)}


def fetch_yfinance_news() -> list:
    try:
        import yfinance as yf
        raw = yf.Ticker("6181.HK").news or []
        items = []
        for n in raw[:15]:
            title = n.get("title", "").strip()
            if not title:
                continue
            ts = n.get("providerPublishTime", 0)
            pub = datetime.fromtimestamp(ts, tz=CST) if ts else None
            items.append(_make_news_item(title, n.get("publisher", "Yahoo Finance"),
                                         n.get("link", ""), pub, "yfinance"))
        print(f"  yfinance æ–°é—»ï¼š{len(items)} æ¡")
        return items
    except Exception as e:
        print(f"  [WARN] yfinance æ–°é—»ï¼š{e}")
        return []


def fetch_eastmoney_news() -> list:
    try:
        import requests
        url = ("https://np-listapi.eastmoney.com/comm/web/getListInfo"
               "?client=web&type=1&mTypeAndCode=128.6181&pageSize=10&pageIndex=1")
        data = requests.get(url, headers={"User-Agent": HEADERS_BROWSER["User-Agent"],
                                          "Referer": "https://quote.eastmoney.com/"},
                            timeout=10).json()
        items = []
        for art in (data.get("data", {}) or {}).get("list", []):
            title = art.get("title", "").strip()
            if not title:
                continue
            pub_str = art.get("publishTime", "") or art.get("ctime", "")
            try:
                pub = datetime.fromisoformat(pub_str.replace("T", " ").split("+")[0])
                pub = pub.replace(tzinfo=CST)
            except Exception:
                pub = None
            items.append(_make_news_item(title, art.get("mediaName", "ä¸œæ–¹è´¢å¯Œ"),
                                         art.get("url", ""), pub, "eastmoney"))
        print(f"  ä¸œæ–¹è´¢å¯Œ æ–°é—»ï¼š{len(items)} æ¡")
        return items
    except Exception as e:
        print(f"  [WARN] ä¸œæ–¹è´¢å¯Œï¼š{e}")
        return []


def fetch_sina_finance_news() -> list:
    try:
        import requests
        from bs4 import BeautifulSoup
        url = ("https://search.sina.com.cn/?q=%E8%80%81%E9%93%BA%E9%BB%84%E9%87%91"
               "&range=all&c=news&sort=time&num=10")
        resp = requests.get(url, headers=HEADERS_BROWSER, timeout=10)
        resp.encoding = "utf-8"
        soup = BeautifulSoup(resp.text, "html.parser")
        items = []
        for div in soup.select(".box-result")[:10]:
            a = div.select_one("h2 a") or div.select_one("a")
            if not a:
                continue
            title = a.get_text(strip=True)
            link  = a.get("href", "")
            t_tag = div.select_one(".fgray_time")
            pub_str = t_tag.get_text(strip=True) if t_tag else ""
            try:
                pub = datetime.strptime(pub_str[:16], "%Yå¹´%mæœˆ%dæ—¥%H:%M").replace(tzinfo=CST)
            except Exception:
                pub = None
            items.append(_make_news_item(title, "æ–°æµªè´¢ç»", link, pub, "sina"))
        print(f"  æ–°æµªè´¢ç» æ–°é—»ï¼š{len(items)} æ¡")
        return items
    except Exception as e:
        print(f"  [WARN] æ–°æµªè´¢ç»ï¼š{e}")
        return []


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ä¸‰ã€ç¤¾åª’åŠ¨æ€ï¼ˆ3 ä¸ªæ¥æº â†’ æ³¨å…¥ã€Œç¤¾åª’åŠ¨æ€ã€Tabï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _make_social_item(platform, title, preview, source, link, pub_dt,
                      stats=None) -> dict:
    return {
        "platform": platform,       # "xhs" | "weibo" | "weixin"
        "title":    title,
        "preview":  preview,
        "source":   source,
        "link":     link,
        "pub_dt":   pub_dt,
        "stats":    stats or {},
        "category": classify_text(title + " " + preview),
    }


# â”€â”€ å¾®åš â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_weibo(keyword: str = "è€é“ºé»„é‡‘", max_items: int = 10) -> list:
    """
    å¾®åšæœç´¢ï¼ˆæ— éœ€ç™»å½•ï¼Œç»“æœæœ‰é™ï¼‰
    - æœ‰æ•ˆå†…å®¹ï¼šçƒ­é—¨è®¨è®ºã€ä¹°å®¶æ™’å•ã€è°ƒä»·è¯é¢˜
    """
    try:
        import requests
        from bs4 import BeautifulSoup
        import urllib.parse

        url = (f"https://s.weibo.com/weibo?q={urllib.parse.quote(keyword)}"
               "&typeall=1&suball=1&Refer=g")
        headers = {**HEADERS_BROWSER,
                   "Referer": "https://weibo.com/",
                   "Cookie":  ""}  # æ—  Cookie ä»å¯è·å–éƒ¨åˆ†å…¬å¼€å¸–
        resp = requests.get(url, headers=headers, timeout=12)
        resp.encoding = "utf-8"
        soup = BeautifulSoup(resp.text, "html.parser")

        items = []
        for card in soup.select(".card-wrap")[:max_items]:
            txt_el = card.select_one(".txt")
            if not txt_el:
                continue
            content = txt_el.get_text(separator=" ", strip=True)
            from_el = card.select_one(".from a")
            link    = ""
            pub     = None
            if from_el:
                raw_link = from_el.get("href", "")
                link = ("https:" + raw_link) if raw_link.startswith("//") else raw_link
                # æ—¶é—´æ–‡æœ¬é€šå¸¸æ˜¯ "Xåˆ†é’Ÿå‰" / "XæœˆXæ—¥"
            # ä½œè€…
            author_el = card.select_one(".name")
            author = author_el.get_text(strip=True) if author_el else "å¾®åšç”¨æˆ·"
            # è½¬å‘/èµ
            reposts = card.select_one(".pos .morepop_count")
            stats = {}

            title   = content[:40] + ("â€¦" if len(content) > 40 else "")
            preview = content[:120]

            items.append(_make_social_item(
                "weibo", title, preview, f"@{author}", link, pub, stats
            ))

        print(f"  å¾®åšï¼š{len(items)} æ¡")
        return items
    except Exception as e:
        print(f"  [WARN] å¾®åšï¼š{e}")
        return []


# â”€â”€ æœç‹—å¾®ä¿¡ï¼ˆå…¬ä¼—å·æ–‡ç« ç´¢å¼•ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_sogou_weixin(keyword: str = "è€é“ºé»„é‡‘", max_items: int = 10) -> list:
    """
    æœç‹—å¾®ä¿¡æœç´¢â€”â€”ç´¢å¼•å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œå“ç‰Œå®˜æ–¹å…¬å‘Š / KOL æµ‹è¯„å‡åœ¨æ­¤
    """
    try:
        import requests
        from bs4 import BeautifulSoup
        import urllib.parse

        url = (f"https://weixin.sogou.com/weixin?type=2"
               f"&query={urllib.parse.quote(keyword)}&ie=utf8")
        headers = {**HEADERS_BROWSER, "Referer": "https://weixin.sogou.com/"}
        resp = requests.get(url, headers=headers, timeout=12)
        resp.encoding = "utf-8"
        soup = BeautifulSoup(resp.text, "html.parser")

        items = []
        for art in soup.select(".news-box .news-list li")[:max_items]:
            title_el   = art.select_one("h3 a")
            preview_el = art.select_one("p.txt-info")
            account_el = art.select_one(".account")
            time_el    = art.select_one(".s-p ~ span") or art.select_one("span.s-p")
            if not title_el:
                continue
            title   = title_el.get_text(strip=True)
            link    = title_el.get("href", "")
            preview = preview_el.get_text(strip=True) if preview_el else ""
            account = account_el.get_text(strip=True) if account_el else "å¾®ä¿¡å…¬ä¼—å·"
            pub_str = time_el.get_text(strip=True) if time_el else ""
            try:
                pub = datetime.strptime(pub_str[:10], "%Y-%m-%d").replace(tzinfo=CST)
            except Exception:
                pub = None

            items.append(_make_social_item(
                "weixin", title, preview, account, link, pub
            ))

        print(f"  æœç‹—å¾®ä¿¡ï¼š{len(items)} æ¡")
        return items
    except Exception as e:
        print(f"  [WARN] æœç‹—å¾®ä¿¡ï¼š{e}")
        return []


# â”€â”€ å°çº¢ä¹¦ï¼ˆPlaywright + å¼¹çª—å…³é—­ï¼ŒGitHub Actions ä¸“ç”¨ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_xiaohongshu(keyword: str = "è€é“ºé»„é‡‘", max_items: int = 10) -> list:
    """
    å°çº¢ä¹¦ç¬”è®°æœç´¢ï¼ˆPlaywright æ— å¤´æµè§ˆå™¨ï¼‰
    ç­–ç•¥ï¼šåŠ è½½æœç´¢é¡µ â†’ å¼ºåˆ¶å…³æ‰ç™»å½•å¼¹çª— â†’ æå–å·²æ¸²æŸ“çš„å¡ç‰‡
    åœ¨ GitHub Actionsï¼ˆUS IPï¼‰ç¯å¢ƒä¸‹å¯è·å–åˆå§‹æœç´¢ç»“æœã€‚
    æœ¬åœ°ä¸­å›½ IP å—é™æ—¶è‡ªåŠ¨é™çº§ï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚
    """
    try:
        from playwright.sync_api import sync_playwright, TimeoutError as PWTimeout
        import urllib.parse

        search_url = (
            "https://www.xiaohongshu.com/search_result"
            f"?keyword={urllib.parse.quote(keyword)}&type=51&source=web_search_result_notes"
        )

        with sync_playwright() as pw:
            browser = pw.chromium.launch(
                headless=True,
                args=[
                    "--no-sandbox",
                    "--disable-dev-shm-usage",
                    "--disable-blink-features=AutomationControlled",
                    "--disable-features=VizDisplayCompositor",
                    "--lang=zh-CN",
                ],
            )
            context = browser.new_context(
                user_agent=(
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/124.0.0.0 Safari/537.36"
                ),
                viewport={"width": 1280, "height": 900},
                locale="zh-CN",
                timezone_id="Asia/Shanghai",
                extra_http_headers={"Accept-Language": "zh-CN,zh;q=0.9"},
            )
            # å±è”½ webdriver ç‰¹å¾
            context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                Object.defineProperty(navigator, 'languages', {get: () => ['zh-CN', 'zh']});
                Object.defineProperty(navigator, 'plugins', {get: () => [1,2,3]});
                window.chrome = {runtime: {}};
            """)
            page = context.new_page()

            try:
                page.goto(search_url, wait_until="domcontentloaded", timeout=25000)

                # ç­‰ä¸€ä¼šå„¿è®©å†…å®¹åŠ è½½
                page.wait_for_timeout(3000)

                # å…³é—­ç™»å½•å¼¹çª—ï¼ˆå¤šç§ selector å…¼å®¹ï¼‰
                for close_sel in [
                    ".login-container .close",
                    "[class*='login'] [class*='close']",
                    "[class*='modal'] [class*='close']",
                    ".close-button",
                    "button[aria-label='Close']",
                    ".overlay .close",
                    "[data-v-close]",
                ]:
                    try:
                        btn = page.query_selector(close_sel)
                        if btn and btn.is_visible():
                            btn.click()
                            page.wait_for_timeout(800)
                            break
                    except Exception:
                        pass

                # æŒ‰ Escape ä¹Ÿèƒ½å…³æ‰éƒ¨åˆ†å¼¹çª—
                page.keyboard.press("Escape")
                page.wait_for_timeout(1000)

                # ç­‰å¾…ç¬”è®°å¡ç‰‡
                card_sel = None
                for sel in ["section.note-item", "div.note-item",
                            "[class*='NoteItem']", ".feeds-page .note-item",
                            ".search-feed-item", "[data-note-id]"]:
                    try:
                        page.wait_for_selector(sel, timeout=5000)
                        card_sel = sel
                        break
                    except PWTimeout:
                        pass

                if not card_sel:
                    print("  å°çº¢ä¹¦ï¼šæœªæ‰¾åˆ°ç¬”è®°å¡ç‰‡ï¼ˆå¯èƒ½éœ€è¦ç™»å½•ï¼‰")
                    return []

                # æŠ“æ›´å¤šå¡ç‰‡ï¼Œè¿‡æ»¤åä»èƒ½ä¿ç•™è¶³å¤Ÿæ•°é‡
                cards = page.query_selector_all(card_sel)[:max_items * 4]
                items = []
                for card in cards:
                    if len(items) >= max_items:
                        break
                    try:
                        title_el = (
                            card.query_selector("span.title")
                            or card.query_selector(".footer span.title")
                            or card.query_selector("[class*='title']")
                        )
                        title = title_el.inner_text().strip() if title_el else ""
                        if not title:
                            continue

                        link_el = (card.query_selector("a[href*='/explore/']")
                                   or card.query_selector("a"))
                        raw_link = link_el.get_attribute("href") if link_el else ""
                        link = ("https://www.xiaohongshu.com" + raw_link
                                if raw_link.startswith("/") else raw_link)

                        author_el = (card.query_selector(".author span")
                                     or card.query_selector(".nickname"))
                        author = author_el.inner_text().strip() if author_el else ""

                        like_el = (card.query_selector(".like-wrapper .count")
                                   or card.query_selector("[class*='like'] [class*='count']"))
                        likes = like_el.inner_text().strip() if like_el else ""

                        # å…³é”®è¯è¿‡æ»¤ï¼šç¡®ä¿å†…å®¹ä¸è€é“ºé»„é‡‘ç›¸å…³
                        combined = title + " " + author
                        if not any(kw in combined for kw in ["è€é“ºé»„é‡‘", "è€é“º", "é»„é‡‘", "å¤æ³•é‡‘"]):
                            continue

                        summary = smart_summary(title, 50)
                        items.append(_make_social_item(
                            "xhs", title, summary,
                            "å°çº¢ä¹¦", link, None,
                            {"likes": likes} if likes else {},
                        ))
                    except Exception:
                        continue

                # æŒ‰çƒ­é—¨ï¼ˆç‚¹èµæ•°ï¼‰é™åºæ’åˆ—ï¼Œå– top6
                items.sort(key=lambda x: parse_likes(x["stats"].get("likes", "")), reverse=True)
                items = items[:6]
                print(f"  å°çº¢ä¹¦ï¼š{len(items)} æ¡ï¼ˆè¿‡æ»¤+çƒ­é—¨æ’åºï¼‰")
                return items

            finally:
                browser.close()

    except ImportError:
        print("  [INFO] å°çº¢ä¹¦ï¼šplaywright æœªå®‰è£…ï¼Œè·³è¿‡")
        return []
    except Exception as e:
        print(f"  [WARN] å°çº¢ä¹¦ï¼š{e}")
        return []


# â”€â”€ å°çº¢ä¹¦å¤‡ç”¨ï¼šæœç‹—æœç´¢ XHS ç›¸å…³å†…å®¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_xhs_via_sogou(keyword: str = "è€é“ºé»„é‡‘ å°çº¢ä¹¦", max_items: int = 6) -> list:
    """
    å½“ Playwright æŠ“ä¸åˆ° XHS å†…å®¹æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆï¼š
    æœç‹—ç½‘é¡µæœç´¢ 'è€é“ºé»„é‡‘ å°çº¢ä¹¦'ï¼Œè¿”å›æåˆ°å°çº¢ä¹¦çš„æ–‡ç« 
    """
    try:
        import requests
        from bs4 import BeautifulSoup
        import urllib.parse

        url = f"https://www.sogou.com/web?query={urllib.parse.quote(keyword)}&num=10"
        headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/124.0.0.0 Safari/537.36"
            ),
            "Accept-Language": "zh-CN,zh;q=0.9",
            "Referer": "https://www.sogou.com/",
        }
        resp = requests.get(url, headers=headers, timeout=12)
        resp.encoding = "utf-8"
        soup = BeautifulSoup(resp.text, "html.parser")

        items = []
        for r in soup.select(".vrwrap"):
            if len(items) >= max_items:
                break
            a = r.select_one("h3 a") or r.select_one("a[href]")
            snippet_el = r.select_one(".str_info") or r.select_one("p")
            if not a:
                continue
            title   = a.get_text(strip=True)
            link    = a.get("href", "")
            if link.startswith("/link?"):
                link = "https://www.sogou.com" + link
            raw_preview = snippet_el.get_text(strip=True) if snippet_el else ""
            if not title:
                continue
            # è¿‡æ»¤ï¼šæ ‡é¢˜æˆ–æ‘˜è¦å¿…é¡»åŒ…å«ã€Œè€é“ºé»„é‡‘ã€
            if "è€é“ºé»„é‡‘" not in title and "è€é“ºé»„é‡‘" not in raw_preview:
                continue
            # 50 å­—ä»¥å†…ç²¾ç‚¼æ‘˜è¦ï¼›è‹¥æœç´¢æ‘˜è¦ä¸ºç©ºåˆ™ä»æ ‡é¢˜æå–
            preview = smart_summary(raw_preview, 50) or smart_summary(title, 50)
            items.append(_make_social_item(
                "xhs", title, preview, "å°çº¢ä¹¦Â·æœç‹—ç´¢å¼•", link, None
            ))

        print(f"  å°çº¢ä¹¦å¤‡ç”¨ï¼ˆæœç‹—ï¼‰ï¼š{len(items)} æ¡")
        return items

    except Exception as e:
        print(f"  [WARN] å°çº¢ä¹¦å¤‡ç”¨ï¼š{e}")
        return []


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  å››ã€é€šç”¨å·¥å…·
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def parse_likes(s: str) -> int:
    """å°† '8.9ä¸‡' / '475' / '2.9ä¸‡' ç»Ÿä¸€è½¬ä¸ºæ•´æ•°ï¼Œç”¨äºçƒ­é—¨æ’åº"""
    if not s:
        return 0
    s = s.strip().replace(",", "")
    try:
        if "ä¸‡" in s:
            return int(float(s.replace("ä¸‡", "")) * 10000)
        return int(s)
    except ValueError:
        return 0


def smart_summary(text: str, max_len: int = 50) -> str:
    """
    ä»æ–‡ç« æ‘˜è¦/æ ‡é¢˜ä¸­æå–ä¸è¶…è¿‡ max_len ä¸ªæ±‰å­—çš„å…³é”®ä¿¡æ¯ï¼š
    - å»é™¤æ—¥æœŸå‰ç¼€ï¼ˆ"6å¤©å‰ -"ã€"2å¹´å‰ -"ã€"2025å¹´3æœˆ2æ—¥ -" ç­‰ï¼‰
    - å»é™¤å°¾éƒ¨æ¥æºæ ‡ç­¾ï¼ˆ"_å“ç‰Œ_å¸‚åœº"ã€"|è…¾è®¯æ–°é—»" ç­‰ï¼‰
    - å°½é‡åœ¨å¥å·/é€—å·å¤„æˆªæ–­ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´
    """
    if not text:
        return ""
    text = text.strip()
    # å»é™¤æ—¶é—´å‰ç¼€ï¼ˆæ”¯æŒ å¤©/å°æ—¶/åˆ†é’Ÿ/æœˆ/å¹´ å‰ï¼‰
    text = re.sub(r"^\d{4}å¹´\d+æœˆ\d+æ—¥\s*[-Â·]\s*", "", text)
    text = re.sub(r"^\d+\s*[å¤©å°æ—¶åˆ†é’Ÿæœˆå¹´]+å‰\s*[-Â·]\s*", "", text)
    # å»é™¤å°¾éƒ¨æ‰€æœ‰ _xxx æˆ– |xxx å¼æ¥æº/åˆ†ç±»æ ‡ç­¾ï¼ˆå¯èƒ½æœ‰å¤šä¸ªï¼‰
    text = re.sub(r"[_|][^_|ã€‚ï¼ï¼Ÿï¼Œ\n]+$", "", text)
    text = re.sub(r"([_|][^_|ã€‚ï¼ï¼Ÿï¼Œ\n]+)+$", "", text)
    # å»é™¤æœ«å°¾ "-æ¥æº" å½¢å¼ï¼ˆå¦‚ "-ä»Šæ—¥å¤´æ¡"ï¼‰
    text = re.sub(r"\s*[-â€“â€”]\s*[\u4e00-\u9fa5a-zA-Z]{2,10}$", "", text)
    # å»é™¤çœç•¥å·æœ«å°¾
    text = re.sub(r"\.{3,}$|â€¦+$", "", text)
    text = text.strip("_|â€¦ \t")
    if not text:
        return ""
    if len(text) <= max_len:
        return text
    # ä¼˜å…ˆåœ¨æ ‡ç‚¹å¤„æˆªæ–­ï¼Œä¿æŒå¥æ„å®Œæ•´ï¼ˆå«è‹±æ–‡é€—å·/å¥å·ï¼‰
    for punct in ["ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", "ã€", ",", "."]:
        idx = text[:max_len].rfind(punct)
        if idx > max_len // 3:
            return text[: idx + 1]
    return text[:max_len] + "â€¦"


def classify_text(text: str) -> str:
    t = text.lower()
    if any(kw in t for kw in KEYWORDS_ADJUST):  return "adjust"
    if any(kw in t for kw in KEYWORDS_PROMO):   return "promo"
    if any(kw in t for kw in KEYWORDS_FINANCE): return "finance"
    return "general"


def merge_dedupe(sources: list[list], key_len: int = 20) -> list:
    """åˆå¹¶å¤šæºã€æŒ‰æ ‡é¢˜å‰Nå­—å»é‡ã€æŒ‰æ—¶é—´å€’åº"""
    seen, merged = set(), []
    for src in sources:
        for item in src:
            key = item["title"][:key_len]
            if key not in seen:
                seen.add(key)
                merged.append(item)
    merged.sort(
        key=lambda x: x.get("pub_dt") or datetime.min.replace(tzinfo=CST),
        reverse=True,
    )
    return merged


def esc(text: str) -> str:
    """HTML è½¬ä¹‰ï¼ˆé˜² XSSï¼‰"""
    return html_lib.escape(str(text))


def rel_time(dt) -> str:
    """å°† datetime è½¬ä¸ºç›¸å¯¹æ—¶é—´å­—ç¬¦ä¸²"""
    if not dt:
        return "â€”"
    now = datetime.now(CST)
    diff = now - dt.astimezone(CST)
    minutes = int(diff.total_seconds() / 60)
    if minutes < 1:   return "åˆšåˆš"
    if minutes < 60:  return f"{minutes}åˆ†é’Ÿå‰"
    hours = minutes // 60
    if hours < 24:    return f"{hours}å°æ—¶å‰"
    days = hours // 24
    if days < 7:      return f"{days}å¤©å‰"
    return dt.astimezone(CST).strftime("%m-%d")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  äº”ã€HTML ç‰‡æ®µæ„å»º
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ è´¢ç»æ–°é—» â†’ ä»·æ ¼/ä¿ƒé”€ Tab â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

TAG_MAP = {
    "adjust":  '<span class="news-tag news-tag-adjust">âš¡ è°ƒä»·</span>',
    "promo":   '<span class="news-tag news-tag-promo">ğŸ ä¿ƒé”€</span>',
    "finance": '<span class="news-tag news-tag-finance">ğŸ“Š è´¢åŠ¡</span>',
    "general": '<span class="news-tag news-tag-general">ğŸ“° èµ„è®¯</span>',
}


def build_alert_bar_text(news: list) -> str:
    adj   = [n for n in news if n["category"] == "adjust"]
    promo = [n for n in news if n["category"] == "promo"]
    if adj:
        return f"âš¡ {esc(adj[0]['title'][:30])} Â· ç‚¹å‡»ã€Œä»·æ ¼æƒ…æŠ¥ã€æŸ¥çœ‹è¯¦æƒ…"
    if promo:
        return f"ğŸ {esc(promo[0]['title'][:30])} Â· ç‚¹å‡»ã€Œé—¨åº—ä¿ƒé”€ã€æŸ¥çœ‹è¯¦æƒ…"
    if news:
        return f"ğŸ“° æœ€æ–°ï¼š{esc(news[0]['title'][:28])}"
    return f"é‡‘ä»·æ•°æ®å·²æ›´æ–° Â· {datetime.now(CST).strftime('%Y-%m-%d %H:%M')} CST"


def build_price_alert_card(news: list, market: dict) -> str:
    adj = [n for n in news if n["category"] == "adjust"]
    now = datetime.now(CST)
    if adj:
        rows = []
        for n in adj[:3]:
            dt_str = n["pub_dt"].strftime("%m-%d %H:%M") if n["pub_dt"] else "â€”"
            link   = esc(n.get("link", ""))
            t      = (f'<a href="{link}" target="_blank" style="color:#E74C3C;'
                      f'text-decoration:none">{esc(n["title"])}</a>'
                      if link else esc(n["title"]))
            rows.append(f'<strong>âš¡ {t}</strong><br>'
                        f'<span style="font-size:10px;color:rgba(231,76,60,0.7)">'
                        f'æ¥æºï¼š{esc(n["source"])} Â· {dt_str}</span>')
        return ('<div class="alert-card">' + "<br><br>".join(rows) + '</div>')

    gc = market.get("gold_change", 0)
    if abs(gc) >= 2:
        msg = (f'é‡‘ä»·ä»Šæ—¥{"ä¸Šæ¶¨" if gc > 0 else "ä¸‹è·Œ"} {abs(gc):.1f}%ï¼Œ'
               f'{"å…³æ³¨æ˜¯å¦è§¦å‘å“ç‰Œè°ƒä»·ã€‚" if gc > 0 else "æš‚æ— è°ƒä»·ä¿¡å·ã€‚"}')
        return (f'<div class="alert-card"><strong>ğŸ“Š é‡‘ä»·åŠ¨æ€</strong><br>{msg}<br>'
                f'<span style="font-size:10px;color:rgba(231,76,60,0.7)">'
                f'è‡ªåŠ¨ç›‘æµ‹ Â· {now.strftime("%Y-%m-%d %H:%M")} CST</span></div>')
    return ""


def build_promo_alert_block(news: list) -> str:
    adj     = [n for n in news if n["category"] == "adjust"]
    promo   = [n for n in news if n["category"] == "promo"]
    others  = [n for n in news if n["category"] in ("finance", "general")][:8]
    now     = datetime.now(CST)
    cards   = []

    def _news_card(items, color_hex, bg_alpha, title_emoji, label):
        if not items:
            return ""
        rows = []
        for n in items[:2]:
            dt = n["pub_dt"].strftime("%m-%d %H:%M") if n["pub_dt"] else "â€”"
            lnk = esc(n.get("link", ""))
            t   = (f'<a href="{lnk}" target="_blank" style="color:{color_hex};'
                   f'text-decoration:none">{esc(n["title"])}</a>'
                   if lnk else esc(n["title"]))
            rows.append(f'{t}<br><span style="font-size:10px;color:rgba'
                        f'({color_hex[1:3]},{color_hex[3:5]},{color_hex[5:]},0.55);">'
                        f'æ¥æºï¼š{esc(n["source"])} Â· {dt}</span>')
        return (
            f'<div style="margin:0 16px 10px;background:rgba{bg_alpha};'
            f'border:1px solid rgba{bg_alpha.replace("0.07","0.25")};'
            f'border-radius:10px;padding:13px 15px;font-size:13px;'
            f'color:{color_hex};line-height:1.85">'
            f'<strong>{title_emoji} {label}</strong><br>'
            + "<br><br>".join(rows) + '</div>'
        )

    cards.append(_news_card(adj,   "#E74C3C", "(192,57,43,0.07)",  "âš¡", "è°ƒä»·åŠ¨æ€"))
    cards.append(_news_card(promo, "#2ECC71", "(46,204,113,0.07)", "ğŸ", "ä¿ƒé”€åŠ¨æ€"))

    if others:
        rows_html = ""
        for n in others:
            dt  = n["pub_dt"].strftime("%m-%d %H:%M") if n["pub_dt"] else "â€”"
            tag = TAG_MAP.get(n["category"], TAG_MAP["general"])
            lnk = esc(n.get("link", ""))
            t   = (f'<a href="{lnk}" target="_blank">{esc(n["title"])}</a>'
                   if lnk else esc(n["title"]))
            rows_html += (
                f'<div class="news-item">'
                f'<div class="news-item-head">'
                f'<div class="news-item-title">{tag}{t}</div>'
                f'<div class="news-item-meta">{dt}</div>'
                f'</div>'
                f'<div style="font-size:10px;color:var(--gold-dim)">{esc(n["source"])}</div>'
                f'</div>'
            )
        cards.append(
            f'<div class="news-feed">'
            f'<div class="news-feed-title">ğŸ“¡ è‡ªåŠ¨è¿½è¸ª Â· è€é“ºé»„é‡‘æœ€æ–°èµ„è®¯ '
            f'<span style="float:right;font-weight:400">{now.strftime("%m-%d %H:%M")} æ›´æ–°</span></div>'
            + rows_html + '</div>'
        )

    result = "\n".join(c for c in cards if c)
    return result or (
        '<div style="margin:0 16px 10px;padding:12px 15px;background:rgba(154,143,126,0.07);'
        'border-radius:10px;font-size:12px;color:var(--gold-dim);text-align:center">'
        'æš‚æœªè·å–åˆ°æœ€æ–°èµ„è®¯ï¼Œè¯·ç¨ååˆ·æ–°</div>'
    )


# â”€â”€ ç¤¾åª’åŠ¨æ€ â†’ ç¤¾åª’ Tab â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PLATFORM_META = {
    "xhs":    ("ğŸ“• å°çº¢ä¹¦", "xhs"),
    "weibo":  ("ğŸ”µ å¾®åš",   "weibo"),
    "weixin": ("ğŸ’š å¾®ä¿¡",   "weixin"),
}

SP_TAG_MAP = {
    "adjust":  '<span class="sp-tag sp-tag-adjust">âš¡ è°ƒä»·</span>',
    "promo":   '<span class="sp-tag sp-tag-promo">ğŸ ä¿ƒé”€</span>',
    "general": "",
    "finance": "",
}


def build_social_tab_html(social_items: list) -> str:
    if not social_items:
        return (
            '<div class="social-empty">'
            'æš‚æœªæŠ“å–åˆ°ç¤¾åª’å†…å®¹<br>'
            '<span style="font-size:10px">å¯èƒ½åŸå› ï¼šç½‘ç»œè¶…æ—¶ / å¹³å°è®¿é—®é™åˆ¶</span>'
            '</div>'
        )

    parts = []
    now   = datetime.now(CST)

    for item in social_items:
        platform = item.get("platform", "weibo")
        label, css_cls = PLATFORM_META.get(platform, ("ğŸ“°", "weibo"))
        title   = esc(item.get("title", ""))
        preview = esc(item.get("preview", ""))
        source  = esc(item.get("source", ""))
        link    = esc(item.get("link", ""))
        stats   = item.get("stats", {})
        cat_tag = SP_TAG_MAP.get(item.get("category", "general"), "")
        time_str = rel_time(item.get("pub_dt"))

        title_html = (
            f'<a class="sp-title" href="{link}" target="_blank">{cat_tag}{title}</a>'
            if link else f'<span class="sp-title">{cat_tag}{title}</span>'
        )
        likes_html = (f'â¤ï¸ {esc(stats["likes"])}' if stats.get("likes") else "")
        link_html  = (f'<a class="sp-link" href="{link}" target="_blank">æŸ¥çœ‹åŸæ–‡ â†’</a>'
                      if link else "")

        parts.append(
            f'<div class="social-post" data-platform="{css_cls}">'
            f'<div class="sp-header">'
            f'<span class="sp-platform {css_cls}">{label}</span>'
            f'<span class="sp-time">{time_str}</span>'
            f'</div>'
            f'{title_html}'
            + (f'<div class="sp-preview">{preview}</div>' if preview else "")
            + f'<div class="sp-footer">'
            f'<div class="sp-stats">'
            + (likes_html if likes_html else f'<span style="opacity:.5">{source}</span>')
            + f'</div>'
            + link_html
            + f'</div>'
            f'</div>'
        )

    update_str = now.strftime("%m-%d %H:%M")
    header = (
        f'<div style="margin:0 16px 8px;font-size:10px;color:var(--gold-dim)">'
        f'å…± {len(social_items)} æ¡ Â· {update_str} æ›´æ–° Â· '
        f'æ¥æºï¼šå°çº¢ä¹¦ / å¾®åš / å¾®ä¿¡å…¬ä¼—å·</div>'
    )
    return header + "\n".join(parts)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  å…­ã€HTML æœ€ç»ˆç”Ÿæˆ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_html(market: dict, news: list, social: list) -> str:
    tpl_path = os.path.join(os.path.dirname(__file__), "laopu-intel.html")
    with open(tpl_path, "r", encoding="utf-8") as f:
        page = f.read()

    now_cst = datetime.now(CST)
    replacements = {
        "{{GOLD_SPOT}}":   f"${market['gold_spot']}",
        "{{GOLD_NOTE}}":   market["gold_note"],
        "{{GOLD_CNY}}":    market["gold_cny"],
        "{{USD_CNY}}":     market["usd_cny"],
        "{{HK_PRICE}}":    market["hk_price"],
        "{{HK_CHANGE}}":   market["hk_change"],
        "{{HK_COLOR}}":    market["hk_color"],
        "{{UPDATE_DATE}}": now_cst.strftime("%Y-%m-%d"),
        "{{UPDATE_TIME}}": now_cst.strftime("%H:%M"),
        "{{ALERT_BAR_TEXT}}":    build_alert_bar_text(news),
        "{{PRICE_ALERT_CARD}}":  build_price_alert_card(news, market),
        "{{PROMO_ALERT_BLOCK}}": build_promo_alert_block(news),
        "{{SOCIAL_TAB_CONTENT}}": build_social_tab_html(social),
    }
    for k, v in replacements.items():
        page = page.replace(k, str(v))

    page = re.sub(r"\{\{[A-Z_]+\}\}", "â€”", page)
    return page


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ä¸ƒã€ä¸»æµç¨‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    now_cst = datetime.now(CST)
    print(f"[{now_cst.strftime('%Y-%m-%d %H:%M:%S')} CST] å¼€å§‹ç”Ÿæˆè€é“ºé»„é‡‘æƒ…æŠ¥é¡µé¢...")

    # â”€â”€ 1. å¸‚åœºæ•°æ® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n[1/4] è·å–å¸‚åœºä»·æ ¼æ•°æ®...")
    market = fetch_market_data()
    if not market["error"]:
        print(f"  ä¼¦æ•¦é‡‘    : ${market['gold_spot']}/oz  ({market['gold_change']:+.2f}%)")
        print(f"  ä¸Šæµ·é‡‘ä¼°ç®— : Â¥{market['gold_cny']}/g")
        print(f"  USD/CNY   : {market['usd_cny']}")
        print(f"  6181.HK   : HK${market['hk_price']}  ({market['hk_change']}%)")

    # â”€â”€ 2. è´¢ç»æ–°é—» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n[2/4] æŠ“å–è´¢ç»æ–°é—»...")
    news = merge_dedupe([
        fetch_yfinance_news(),
        fetch_eastmoney_news(),
        fetch_sina_finance_news(),
    ])
    cats = {k: sum(1 for n in news if n["category"] == k)
            for k in ("adjust", "promo", "finance", "general")}
    print(f"  åˆå¹¶å»é‡å {len(news)} æ¡ï¼šè°ƒä»· {cats['adjust']} | ä¿ƒé”€ {cats['promo']} | "
          f"è´¢åŠ¡ {cats['finance']} | èµ„è®¯ {cats['general']}")

    # â”€â”€ 3. ç¤¾åª’åŠ¨æ€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n[3/4] æŠ“å–ç¤¾åª’åŠ¨æ€...")
    xhs_items = fetch_xiaohongshu()
    if not xhs_items:
        print("  å°çº¢ä¹¦ Playwright æœªè·å–åˆ°å†…å®¹ï¼Œåˆ‡æ¢å¤‡ç”¨æœç‹—ç´¢å¼•...")
        xhs_items = fetch_xhs_via_sogou()
    social = merge_dedupe([
        fetch_weibo(),
        fetch_sogou_weixin(),
        xhs_items,
    ])
    by_platform = {p: sum(1 for s in social if s["platform"] == p)
                   for p in ("xhs", "weibo", "weixin")}
    print(f"  åˆå¹¶å»é‡å {len(social)} æ¡ï¼š"
          f"å°çº¢ä¹¦ {by_platform['xhs']} | å¾®åš {by_platform['weibo']} | å¾®ä¿¡ {by_platform['weixin']}")

    # â”€â”€ 4. ç”Ÿæˆ HTML â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n[4/4] ç”Ÿæˆ index.html...")
    page = generate_html(market, news, social)
    out  = os.path.join(os.path.dirname(__file__), "index.html")
    with open(out, "w", encoding="utf-8") as f:
        f.write(page)
    print(f"  âœ… index.html å·²ç”Ÿæˆï¼ˆ{len(page):,} å­—èŠ‚ï¼‰")
    print(f"\n[å®Œæˆ] {datetime.now(CST).strftime('%Y-%m-%d %H:%M:%S')} CST")


if __name__ == "__main__":
    main()
